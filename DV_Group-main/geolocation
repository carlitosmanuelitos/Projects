#!pip install geopy pycountry
#!pip install --use-pep517 googlemaps

import pandas as pd
import numpy as np
from geopy.geocoders import GoogleV3
import googlemaps
import boto3
from botocore.exceptions import ClientError
import pycountry

import logging
from tqdm import tqdm
from typing import Tuple, Optional, Dict
from datetime import datetime
from pathlib import Path
import json
import unicodedata
import re
import time
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# GENERATE TESTING DATA
def reduce_csv_files(order_file, pos_file, output_order_file, output_pos_file, percentage=0.05):
    # Load the CSV files with delimiter ';'
    orders_df = pd.read_csv(order_file, delimiter=';')
    pos_df = pd.read_csv(pos_file, delimiter=';')

    # Function to reduce the size of the DataFrame by randomly deleting lines
    def reduce_size(df, percentage):
        # Calculate the number of rows to keep
        num_rows_to_keep = int(len(df) * percentage)
        # Randomly sample the rows to keep
        reduced_df = df.sample(n=num_rows_to_keep, random_state=1)
        return reduced_df

    # Reduce the size of both DataFrames
    reduced_orders_df = reduce_size(orders_df, percentage)
    reduced_pos_df = reduce_size(pos_df, percentage)

    # Save the reduced DataFrames back to CSV files
    reduced_orders_df.to_csv(output_order_file, index=False, sep=';')
    reduced_pos_df.to_csv(output_pos_file, index=False, sep=';')

    print(f"The files have been reduced to {percentage*100}% of their original size and saved as '{output_order_file}' and '{output_pos_file}'.")

# Example usage
reduce_csv_files('CZ_ORDER_DATA.csv', 'CZ_POS_DATA_GUID.csv', 'testing_CZ_ORDER.csv', 'testing_CZ_POS.csv')


# IAM ISSUES & ACCESS POLICIES

def get_current_iam_role():
    # Create an STS client
    sts_client = boto3.client('sts')

    # Get the identity of the caller
    identity = sts_client.get_caller_identity()

    # Print the ARN of the role
    print(f"User ARN: {identity['Arn']}")
    print(f"Account: {identity['Account']}")
    print(f"User ID: {identity['UserId']}")

# Example usage
get_current_iam_role()


# ------- PART0. API KEYS ACCESS MANAGMENT
def get_secret():
    secret_name = "CommerceReporting/GeoLocation"
    region_name = "eu-west-1"

    # Create a Secrets Manager client
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )

    try:
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name
        )
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'AccessDeniedException':
            print("You do not have sufficient access to perform this action. Please check IAM access policies")
        elif error_code == 'NotAuthorized':
            print("You do not have permission to perform this action. Please check IAM access policies")
        else:
            print(f"An unknown error occurred: {e}")
        raise e

    # Parse the secret string into a dictionary
    secret = json.loads(get_secret_value_response['SecretString'])

    geocoding_api_key = secret['google_api']

    # Return the API keys
    return geocoding_api_key

# Example usage
geocoding_api_key = get_secret()
print(f"Geocoding API Key: {geocoding_api_key}")




#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ----------------------------------------------------PART 1 PART 1 PART 1 PART 1 ---------------------------------------------------------------------- #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####


class GeoLocationMapper:
    """A class to handle geolocation mapping using Google Maps API."""
    
    def __init__(self, api_key: str, cache_file: str = 'geolocation_cache.json'):
        """
        Initialize the GeoLocationMapper.
        
        Args:
            api_key (str): Google Maps API key
            cache_file (str): Path to the cache file
        """
        self.api_key = api_key
        self.cache_file = cache_file
        self.geolocator = GoogleV3(api_key=api_key)
        self.cache = self._load_cache()
        self.total_calls = 0
        self.rate_limit_delay = 0.1  # 100ms delay between API calls
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'geolocation_mapping_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )

    def _load_cache(self) -> Dict:
        """Load the geolocation cache from file."""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logging.error(f"Error loading cache: {e}")
            return {}

    def _save_cache(self) -> None:
        """Save the geolocation cache to file."""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"Error saving cache: {e}")

    @staticmethod
    def normalize_text(text: str) -> str:
        """Normalize text to ASCII and remove hidden characters."""
        if not isinstance(text, str):
            return text
        # Normalize to ASCII
        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')
        # Remove hidden characters
        text = ''.join(c for c in text if c.isprintable())
        # Standardize whitespace
        text = re.sub(r'\s+', ' ', text.strip())
        return text

    @staticmethod
    def get_country_name(country_code: str) -> str:
        """Get full country name from country code."""
        try:
            return pycountry.countries.get(alpha_2=country_code).name
        except (AttributeError, KeyError):
            logging.warning(f"Country code '{country_code}' not found")
            return country_code

    def get_lat_long(self, address_components: dict) -> Tuple[Optional[float], Optional[float]]:
        """
        Get latitude and longitude for an address.
        
        Args:
            address_components (dict): Dictionary containing address components
                
        Returns:
            tuple: (latitude, longitude) or (None, None) if not found
        """
        try:
            # Validate required fields
            required_fields = ['street_name', 'postal_code', 'town', 'country_code']
            if not all(address_components.get(field) for field in required_fields):
                missing_fields = [f for f in required_fields if not address_components.get(f)]
                logging.warning(f"Missing required fields: {missing_fields}")
                return None, None

            # Construct address string
            country = self.get_country_name(address_components['country_code'])
            address_parts = [
                f"{address_components['street_name']} {address_components.get('street_number', '')}".strip(),
                f"{address_components['postal_code']}",
                address_components['town'],
                country
            ]
            address = ', '.join(filter(None, address_parts))

            # Check cache
            if address in self.cache:
                logging.debug(f"Cache hit for address: {address}")
                return self.cache[address]

            # Rate limiting
            time.sleep(self.rate_limit_delay)

            # Geocode address
            location = self.geolocator.geocode(address)
            self.total_calls += 1

            if location:
                result = (location.latitude, location.longitude)
                self.cache[address] = result
                if self.total_calls % 100 == 0:  # Save cache periodically
                    self._save_cache()
                return result
            
            logging.warning(f"No location found for address: {address}")
            return None, None

        except Exception as e:
            logging.error(f"Error geocoding address: {e}")
            return None, None

    def process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Process the input DataFrame and add geolocation data.
        
        Args:
            df (pd.DataFrame): Input DataFrame with address information
            
        Returns:
            pd.DataFrame: DataFrame with added latitude and longitude columns
        """
        # Normalize text columns
        text_columns = ['STREET NAME', 'TOWN', 'POSTAL CODE']
        for col in text_columns:
            if col in df.columns:
                df[col] = df[col].apply(self.normalize_text)

        # Create progress bar
        tqdm.pandas(desc="Mapping geolocations")

        # Apply geolocation mapping
        results = df.progress_apply(
            lambda row: self.get_lat_long({
                'street_name': row.get('STREET NAME'),
                'street_number': row.get('STREET NUMBER'),
                'postal_code': row.get('POSTAL CODE'),
                'town': row.get('TOWN'),
                'country_code': row.get('COUNTRY')
            }), 
            axis=1
        )
        
        df['Latitude'], df['Longitude'] = zip(*results)
        return df

    def map_geolocations(self, input_file: str, output_file: str) -> None:
        """
        Main function to process input file and save results.
        
        Args:
            input_file (str): Path to input CSV file
            output_file (str): Path to output Excel file
        """
        try:
            # Read input file
            logging.info(f"Reading input file: {input_file}")
            data = pd.read_csv(input_file, delimiter=';', encoding='utf-8')
            
            # Process data
            df = self.process_dataframe(data)
            
            # Create unmapped addresses DataFrame
            unmapped_df = df[df['Latitude'].isna() | df['Longitude'].isna()]
            
            # Save results
            with pd.ExcelWriter(output_file) as writer:
                df.to_excel(writer, sheet_name='Coordinates', index=False)
                unmapped_df.to_excel(writer, sheet_name='Unmapped', index=False)
            
            # Log statistics
            self._log_statistics(df, unmapped_df)
            
            # Save final cache
            self._save_cache()
            
        except Exception as e:
            logging.error(f"Error processing file: {e}")
            raise

    def _log_statistics(self, df: pd.DataFrame, unmapped_df: pd.DataFrame) -> None:
        """Log processing statistics."""
        unmapped_count = len(unmapped_df)
        total_rows = len(df)
        unmapped_percentage = (unmapped_count / total_rows) * 100 if total_rows > 0 else 0
        
        logging.info(f"Processing completed:")
        logging.info(f"Total addresses processed: {total_rows}")
        logging.info(f"Successfully mapped: {total_rows - unmapped_count}")
        logging.info(f"Failed to map: {unmapped_count} ({unmapped_percentage:.2f}%)")
        logging.info(f"Unique addresses in cache: {len(self.cache)}")
        logging.info(f"Total API calls made: {self.total_calls}")


# Example usage:
mapper = GeoLocationMapper(api_key=geocoding_api_key)
mapper.map_geolocations(
    input_file='testing_DE_ORDER.csv',
    output_file='testing_DE_ORDER_mapped.xlsx'
)


#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ----------------------------------------------------PART 2 PART 2 PART 2 PART 2 ---------------------------------------------------------------------- #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####
#### ------------------------------------------------------------------------------------------------------------------------------------------------------ #####


import pandas as pd
import numpy as np
from googlemaps import Client
import logging
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import json
from typing import Dict, List, Tuple, Optional
import time
from concurrent.futures import ThreadPoolExecutor


class StoreDistanceMapper:
    """Handles mapping of orders to stores with distance calculations."""
    
    def __init__(self, api_key: str, cache_file: str = 'distance_cache.json'):
        """Initialize the mapper with API key and cache file."""
        self.gmaps = Client(key=api_key)
        self.cache_file = cache_file
        self.distance_cache = self._load_cache()
        self.api_calls = 0
        self.cache_hits = 0
        
        # Configure logging
        log_file = f'store_mapping_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

    def _load_cache(self) -> Dict:
        """Load distance cache from file."""
        try:
            if Path(self.cache_file).exists():
                with open(self.cache_file, 'r') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logging.error(f"Error loading cache: {e}")
            return {}

    def _save_cache(self) -> None:
        """Save distance cache to file."""
        try:
            with open(self.cache_file, 'w') as f:
                json.dump(self.distance_cache, f)
        except Exception as e:
            logging.error(f"Error saving cache: {e}")

    @staticmethod
    def haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate haversine distance between two points."""
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        dlat, dlon = lat2 - lat1, lon2 - lon1
        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
        return 6367 * 2 * np.arcsin(np.sqrt(a))

    def get_drivable_distance(self, origin: Tuple[float, float], 
                            destination: Tuple[float, float]) -> Optional[float]:
        """Get drivable distance between two points with caching."""
        cache_key = f"{origin}_{destination}"
        
        if cache_key in self.distance_cache:
            self.cache_hits += 1
            logging.debug(f"Cache hit for distance: {cache_key}")
            return self.distance_cache[cache_key]
        
        try:
            time.sleep(0.1)  # Rate limiting
            self.api_calls += 1
            result = self.gmaps.distance_matrix(
                origins=[origin],
                destinations=[destination],
                mode='driving',
                units='metric'
            )
            
            if result['status'] == 'OK':
                distance = result['rows'][0]['elements'][0]['distance']['value'] / 1000
                self.distance_cache[cache_key] = distance
                
                # Periodic cache saving
                if self.api_calls % 100 == 0:
                    self._save_cache()
                    
                return distance
            else:
                logging.warning(f"API returned non-OK status: {result['status']}")
                return None
                
        except Exception as e:
            logging.error(f"Error calculating drivable distance: {e}")
            return None

    def _format_closest_store_row(self, order: pd.Series, store_info: Dict) -> Dict:
        """Format the closest store information into a row dictionary."""
        return {
            'DATE': order.get('DATE', ''),
            'COUNTRY': order.get('COUNTRY', ''),
            'ORDER': order.get('ORDER', ''),
            'CART CODE': order.get('CART CODE', ''),
            'ORDERCHANNEL': order.get('ORDERCHANNEL', ''),
            'ORDERTYPE': order.get('ORDERTYPE', ''),
            'PMIORDERSTATUS': order.get('PMIORDERSTATUS', ''),
            'BASE STORE': order.get('BASE STORE', ''),
            'DELIVERY MODE': order.get('Delivery Mode', ''),
            'DELIVERY STATUS': order.get('Delivery Status', ''),
            'CARRIER': order.get('CARRIER', ''),
            'SUBTOTAL': order.get('SUBTOTAL', ''),
            'TOWN': order.get('TOWN', ''),
            'POSTAL CODE': order.get('POSTAL CODE', ''),
            'STREET NAME': order.get('STREET NAME', ''),
            'STREET NUMBER': order.get('STREET NUMBER', ''),
            'Latitude': order['Latitude'],
            'Longitude': order['Longitude'],
            'Closest Store': store_info['store']['displayname'],
            'Linear Distance in KM': round(store_info['linear_distance'], 2),
            'Drivable Distance in KM': round(store_info['drivable_distance'], 2) if store_info['drivable_distance'] else None,
            'POS Latitude': store_info['store']['latitude'],
            'POS Longitude': store_info['store']['longitude']
        }

    def _format_nearby_stores_rows(self, order: pd.Series, stores: List[Dict], distance_type: str) -> List[Dict]:
        """Format nearby stores information into row dictionaries."""
        formatted_rows = []
        for store_info in stores:
            row = {
                'ORDER': order['ORDER'],
                'Order Address': f"{order.get('COUNTRY', '')}, {order.get('TOWN', '')}, {order.get('POSTAL CODE', '')}, {order.get('STREET NAME', '')}, {order.get('STREET NUMBER', '')}",
                'Latitude': order['Latitude'],
                'Longitude': order['Longitude'],
                'POS': store_info['store']['displayname'],
                'POS Latitude': store_info['store']['latitude'],
                'POS Longitude': store_info['store']['longitude']
            }
            
            if distance_type == 'linear':
                row['Linear Distance'] = round(store_info['linear_distance'], 2)
            else:  # drivable
                row['Drivable Distance'] = round(store_info['drivable_distance'], 2)
            
            formatted_rows.append(row)
            
        return formatted_rows

    def _update_store_counts(self, closest_df: pd.DataFrame, 
                           linear_df: pd.DataFrame, 
                           drivable_df: pd.DataFrame) -> pd.DataFrame:
        """Update the closest store DataFrame with store counts."""
        # Group by ORDER to count stores within range
        linear_counts = linear_df.groupby('ORDER').size().reset_index(name='Stores 5km Linear')
        drivable_counts = drivable_df.groupby('ORDER').size().reset_index(name='Stores 5km Drivable')
        
        # Merge counts with closest store DataFrame
        result_df = closest_df.merge(linear_counts, on='ORDER', how='left')
        result_df = result_df.merge(drivable_counts, on='ORDER', how='left')
        
        # Fill NaN values with 0 (for orders with no stores in range)
        result_df['Stores 5km Linear'] = result_df['Stores 5km Linear'].fillna(0).astype(int)
        result_df['Stores 5km Drivable'] = result_df['Stores 5km Drivable'].fillna(0).astype(int)
        
        return result_df

    def _find_closest_store(self, order: pd.Series, 
                           stores_df: pd.DataFrame) -> Dict:
        """Find the closest store using haversine distance first."""
        min_distance = float('inf')
        closest_store = None
        
        for _, store in stores_df.iterrows():
            distance = self.haversine(
                order['Latitude'], order['Longitude'],
                store['latitude'], store['longitude']
            )
            if distance < min_distance:
                min_distance = distance
                closest_store = store
        
        # Get drivable distance for closest store
        drivable_distance = self.get_drivable_distance(
            (order['Latitude'], order['Longitude']),
            (closest_store['latitude'], closest_store['longitude'])
        )
        
        return {
            'store': closest_store,
            'linear_distance': min_distance,
            'drivable_distance': drivable_distance
        }

    def _find_stores_within_linear(self, order: pd.Series, 
                                 stores_df: pd.DataFrame) -> List[Dict]:
        """Find all stores within 5km linear distance."""
        nearby_stores = []
        
        for _, store in stores_df.iterrows():
            distance = self.haversine(
                order['Latitude'], order['Longitude'],
                store['latitude'], store['longitude']
            )
            if distance <= 5:
                nearby_stores.append({
                    'store': store,
                    'linear_distance': distance
                })
        
        return nearby_stores

    def _get_drivable_stores(self, order: pd.Series, 
                            linear_stores: List[Dict]) -> List[Dict]:
        """Get drivable distances for stores within linear range."""
        drivable_stores = []
        
        for store_info in linear_stores:
            drivable_distance = self.get_drivable_distance(
                (order['Latitude'], order['Longitude']),
                (store_info['store']['latitude'], store_info['store']['longitude'])
            )
            
            if drivable_distance is not None and drivable_distance <= 5:
                store_info['drivable_distance'] = drivable_distance
                drivable_stores.append(store_info)
        
        return drivable_stores

    def _create_output_sheets(self, closest_stores: List[Dict],
                            linear_stores: List[Dict],
                            drivable_stores: List[Dict],
                            output_file: str) -> None:
        """Create and save all output sheets."""
        # Create DataFrames
        closest_df = pd.DataFrame(closest_stores)
        linear_df = pd.DataFrame(linear_stores)
        drivable_df = pd.DataFrame(drivable_stores)
        
        # Update store counts
        closest_df = self._update_store_counts(closest_df, linear_df, drivable_df)
        
        # Save to Excel
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            closest_df.to_excel(writer, sheet_name='Coordinates', index=False)
            linear_df.to_excel(writer, sheet_name='5km Stores Linear', index=False)
            drivable_df.to_excel(writer, sheet_name='5km Stores Drivable', index=False)

    def process_orders(self, orders_df: pd.DataFrame, 
                      stores_df: pd.DataFrame, output_file: str) -> None:
        """Process orders and create output sheets."""
        try:
            # Initialize result DataFrames
            closest_stores = []
            stores_within_5km_linear = []
            stores_within_5km_drivable = []
            
            # Process each order
            for idx, order in tqdm(orders_df.iterrows(), total=len(orders_df), 
                                 desc="Processing orders"):
                if pd.isna(order['Latitude']) or pd.isna(order['Longitude']):
                    continue
                
                order_location = (order['Latitude'], order['Longitude'])
                
                # Find closest store and stores within 5km
                closest_store_info = self._find_closest_store(order, stores_df)
                linear_stores = self._find_stores_within_linear(order, stores_df)
                drivable_stores = self._get_drivable_stores(order, linear_stores)
                
                # Update results
                closest_stores.append(self._format_closest_store_row(order, closest_store_info))
                stores_within_5km_linear.extend(self._format_nearby_stores_rows(order, linear_stores, 'linear'))
                stores_within_5km_drivable.extend(self._format_nearby_stores_rows(order, drivable_stores, 'drivable'))
            
            # Create output DataFrames
            self._create_output_sheets(
                closest_stores,
                stores_within_5km_linear,
                stores_within_5km_drivable,
                output_file
            )
            
            logging.info(f"Processing completed. API calls: {self.api_calls}, Cache hits: {self.cache_hits}")
            self._save_cache()
            
        except Exception as e:
            logging.error(f"Error in process_orders: {e}")
            raise


def main():
    """Main execution function."""
    try:
        # Configuration
        api_key = geocoding_api_key
        orders_file = 'DE_ORDER_DATA_geo.xlsx'
        stores_file = 'DE_POS_DATA_GUID_brand.csv'
        output_file = 'F_DE_distance_mapping_results.xlsx'
        
        # Load data
        orders_df = pd.read_excel(orders_file)
        stores_df = pd.read_csv(stores_file, delimiter=';', encoding='utf-8')
        
        # Initialize and run mapper
        mapper = StoreDistanceMapper(api_key)
        mapper.process_orders(orders_df, stores_df, output_file)
        
    except Exception as e:
        logging.error(f"Error in main execution: {e}")
        raise

if __name__ == "__main__":
    main()


3. Introduction
3.1. Assumptions
3.2. Technical context
3.3. Objective
3.4. Impacted components
4. Concept
5. Integration
5.1. Summary
5.2. Perf. KPI section for 3rd party integration & Observability matrix
5.3. Section 2
5.4. Section 3
6. APIs








# Store Fulfillment Analysis System
## Comprehensive Solution Design Document

## 1. Introduction

### 1.1 Executive Summary
This document outlines the technical design for a Store Fulfillment Analysis System that evaluates the geographical relationship between customer orders and store locations. The system provides critical insights for optimizing store network coverage and improving order fulfillment capabilities.

### 1.2 Business Context
- **Primary Business Need**: Evaluate store network coverage for order fulfillment
- **Key Business Questions**:
  - Which stores are closest to each order?
  - How many stores are within fulfillment range?
  - What is the actual driving distance versus straight-line distance?
  - Are there areas with insufficient store coverage?

### 1.3 Assumptions
#### 1.3.1 Data Assumptions
- Order data contains valid delivery addresses
- Store data includes accurate geographical coordinates
- Address formats are relatively standardized within each country
- Data volumes are manageable within API rate limits

#### 1.3.2 Technical Assumptions
- Google Maps API access is available with sufficient quota
- AWS infrastructure is accessible for secret management
- Local processing power is sufficient for data volume
- Network connectivity is reliable for API calls

#### 1.3.3 Business Assumptions
- 5KM is the standard fulfillment range requirement
- Both linear and driving distances are relevant for analysis
- Store operating hours and capacity are not initial constraints

### 1.4 Technical Context
#### 1.4.1 System Environment
- Python 3.x runtime environment
- AWS cloud infrastructure
- Local processing capabilities
- File system storage for caching

#### 1.4.2 Dependencies
- External APIs:
  - Google Maps Geocoding API
  - Google Maps Distance Matrix API
- AWS Services:
  - AWS Secrets Manager
  - IAM for access control
- Python Libraries:
  - pandas for data processing
  - numpy for mathematical calculations
  - boto3 for AWS integration

### 1.5 Objectives
#### 1.5.1 Primary Objectives
1. Accurate geocoding of order addresses
2. Efficient mapping of orders to nearby stores
3. Precise distance calculations using multiple methods
4. Comprehensive coverage analysis

#### 1.5.2 Technical Objectives
1. Minimize API calls through optimization
2. Maintain high performance with large datasets
3. Ensure reliable error handling
4. Provide detailed logging and monitoring
5. Enable easy maintenance and updates

## 2. Architecture Assessment

### 2.1 System Architecture
#### 2.1.1 High-Level Components
```
[Input Layer]
    │
    ├── Order Data Processing
    │   ├── Address Normalization
    │   ├── Geocoding
    │   └── Validation
    │
    ├── Store Data Processing
    │   ├── Coordinate Validation
    │   └── Store Information Management
    │
[Processing Layer]
    │
    ├── Distance Calculation Engine
    │   ├── Haversine Calculator
    │   ├── Distance Matrix Service
    │   └── Optimization Logic
    │
    ├── Store Mapping Engine
    │   ├── Proximity Analysis
    │   ├── Coverage Calculation
    │   └── Store Selection Logic
    │
[Output Layer]
    │
    ├── Results Generation
    │   ├── Excel Report Creation
    │   ├── Statistics Calculation
    │   └── Data Visualization
```

#### 2.1.2 Component Interactions
1. **Data Flow**
   ```
   Raw Data → Normalization → Geocoding → Distance Calculation → Analysis → Reporting
   ```

2. **Service Integration**
   ```
   AWS Secrets → API Keys → Google Services → Results → Local Storage
   ```

### 2.2 Optimization Strategy

#### 2.2.1 API Call Reduction Framework
1. **Primary Strategy**: Haversine First Approach
   - Calculate linear distances for all stores
   - Only calculate driving distances for stores within extended linear range
   - Use cache for repeated calculations

2. **Implementation Logic**:
```python
def optimize_distance_calculations(order, stores):
    # Step 1: Calculate Haversine distances for all stores
    linear_distances = calculate_linear_distances(order, stores)
    
    # Step 2: Filter stores within extended range (e.g., 7km linear)
    potential_stores = filter_stores_within_range(linear_distances, extended_range=7)
    
    # Step 3: Calculate driving distances only for filtered stores
    driving_distances = calculate_driving_distances(order, potential_stores)
    
    # Step 4: Final filtering for 5km requirement
    final_stores = filter_stores_within_range(driving_distances, target_range=5)
    
    return final_stores
```

#### 2.2.2 Caching Strategy
1. **Two-Level Caching**
   - Memory cache for frequent calculations
   - Persistent file cache for session retention
   - Periodic cache updates and cleanup

2. **Cache Structure**:
```json
{
    "geocoding": {
        "address_hash": {
            "lat": float,
            "lng": float,
            "timestamp": datetime
        }
    },
    "distances": {
        "origin_dest_hash": {
            "linear": float,
            "driving": float,
            "timestamp": datetime
        }
    }
}
```

## 3. Integration Details

### 3.1 Data Processing Pipeline

#### 3.1.1 Address Processing
1. **Normalization Steps**
   - Remove special characters
   - Standardize format
   - Validate required fields
   - Handle edge cases

2. **Geocoding Process**
   - Address validation
   - API call optimization
   - Error handling
   - Result validation

#### 3.1.2 Distance Calculation
1. **Haversine Calculation**
```python
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371  # Earth's radius in kilometers
    
    # Convert coordinates to radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    
    # Haversine formula
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    
    return R * c
```

2. **Distance Matrix Implementation**
```python
def get_driving_distance(origin, destination, client):
    try:
        result = client.distance_matrix(
            origins=[origin],
            destinations=[destination],
            mode='driving',
            units='metric'
        )
        return process_distance_result(result)
    except Exception as e:
        handle_api_error(e)
```

### 3.2 Performance Optimization

#### 3.2.1 Key Performance Indicators
1. **API Usage Metrics**
   - Calls per minute
   - Cache hit ratio
   - Error rate
   - Response time distribution

2. **Processing Metrics**
   - Records processed per second
   - Memory usage
   - CPU utilization
   - I/O operations

#### 3.2.2 Optimization Techniques
1. **Batch Processing**
   - Group similar requests
   - Optimize API payload size
   - Balance batch size vs. latency

2. **Parallel Processing**
   - Multi-threading for I/O operations
   - Process pooling for calculations
   - Resource management

### 3.3 Error Handling and Recovery

#### 3.3.1 Error Categories
1. **Data Errors**
   - Invalid addresses
   - Missing coordinates
   - Format inconsistencies

2. **API Errors**
   - Rate limiting
   - Service unavailability
   - Invalid responses

#### 3.3.2 Recovery Strategies
1. **Automatic Retry**
   - Exponential backoff
   - Alternative service usage
   - Failure logging

2. **Data Recovery**
   - Checkpoint creation
   - Progress tracking
   - Resume capability

## 4. API Specifications

### 4.1 External APIs

#### 4.1.1 Google Maps APIs
1. **Geocoding API**
   - Endpoint: `https://maps.googleapis.com/maps/api/geocode/json`
   - Rate Limits: 50 QPS
   - Error Handling: Built-in retry mechanism

2. **Distance Matrix API**
   - Endpoint: `https://maps.googleapis.com/maps/api/distancematrix/json`
   - Rate Limits: 100 QPS
   - Optimization: Batch requests

### 4.2 Internal APIs

#### 4.2.1 GeoLocationMapper
```python
class GeoLocationMapper:
    def map_geolocations(
        self,
        input_file: str,
        output_file: str
    ) -> None:
        """
        Maps physical addresses to coordinates
        
        Parameters:
        - input_file: Path to CSV with address data
        - output_file: Path for output Excel file
        
        Returns:
        - None. Creates Excel file with mapped coordinates
        """
```

#### 4.2.2 StoreDistanceMapper
```python
class StoreDistanceMapper:
    def process_orders(
        self,
        orders_df: pd.DataFrame,
        stores_df: pd.DataFrame,
        output_file: str
    ) -> None:
        """
        Maps orders to nearby stores
        
        Parameters:
        - orders_df: DataFrame with order data
        - stores_df: DataFrame with store data
        - output_file: Path for output Excel file
        
        Returns:
        - None. Creates Excel file with mapping results
        """
```

## 5. Maintenance and Support

### 5.1 Monitoring and Logging
1. **Log Levels**
   - ERROR: API failures, data errors
   - WARNING: Performance issues, near-limit conditions
   - INFO: Progress updates, successful operations
   - DEBUG: Detailed operation information

2. **Monitoring Metrics**
   - API usage tracking
   - Performance monitoring
   - Error rate tracking
   - Cache effectiveness

### 5.2 Maintenance Procedures
1. **Regular Maintenance**
   - Cache cleanup
   - Log rotation
   - Performance optimization
   - Data validation

2. **Update Procedures**
   - Code updates
   - API version management
   - Configuration updates
   - Documentation maintenance

## 6. Future Enhancements

### 6.1 Potential Improvements
1. **Technical Enhancements**
   - Real-time processing capability
   - Advanced caching mechanisms
   - Machine learning for prediction
   - Additional distance calculation methods

2. **Business Features**
   - Store capacity consideration
   - Operating hours integration
   - Dynamic range adjustment
   - Cost optimization analysis

### 6.2 Scalability Considerations
1. **Vertical Scaling**
   - Processing optimization
   - Memory management
   - Cache optimization

2. **Horizontal Scaling**
   - Distributed processing
   - Load balancing
   - Data partitioning

## 7. Conclusion

### 7.1 Success Criteria
1. **Technical Success Metrics**
   - API call reduction > 50%
   - Cache hit ratio > 80%
   - Processing time < 1s per record
   - Error rate < 1%

2. **Business Success Metrics**
   - Accurate store mapping
   - Comprehensive coverage analysis
   - Actionable insights generation
   - Reliable performance

### 7.2 Risk Mitigation
1. **Technical Risks**
   - API availability
   - Data quality
   - Performance issues
   - System failures

2. **Business Risks**
   - Incorrect analysis
   - Missing coverage
   - Incomplete insights
   - Cost overruns




-----------------------------------------------------------------------------------

# Store Fulfillment Analysis System
## Implementation-Specific Solution Design

## 1. Introduction

### 1.1 Project Overview
This system implements a two-phase solution for store fulfillment analysis:
1. Address Geocoding (GeoLocationMapper)
2. Store Distance Mapping (StoreDistanceMapper)

### 1.2 Assumptions
- Input data format:
  - Orders CSV with columns: DATE, COUNTRY, ORDER, CART CODE, etc.
  - Stores CSV with columns: displayname, latitude, longitude, etc.
  - Delimiter: semicolon (';')
  - UTF-8 encoding
- AWS environment access for API key management
- Google Maps API services availability
- 5km as standard fulfillment radius

### 1.3 Technical Context
- **Environment**
  - Python 3.x
  - AWS Secrets Manager
  - Google Maps API Suite
  
- **Core Dependencies**
  ```python
  import pandas as pd
  import numpy as np
  from geopy.geocoders import GoogleV3
  import googlemaps
  import boto3
  import pycountry
  ```

### 1.4 Objectives
1. Geocode customer delivery addresses
2. Find closest store to each order
3. Identify stores within 5km range using:
   - Linear distance (Haversine)
   - Driving distance (Google Distance Matrix)
4. Generate comprehensive analysis reports

## 2. System Architecture

### 2.1 Component Overview
```
[Access Management]
    │
    ├── AWS Secrets Manager Integration
    │   └── get_secret()
    │
[Data Processing]
    │
    ├── GeoLocationMapper
    │   ├── Address Normalization
    │   ├── Geocoding
    │   └── Cache Management
    │
    └── StoreDistanceMapper
        ├── Distance Calculations
        ├── Store Mapping
        └── Results Generation
```

### 2.2 Core Classes

#### GeoLocationMapper
```python
class GeoLocationMapper:
    def __init__(self, api_key: str, cache_file: str = 'geolocation_cache.json'):
        # Initialization with API key and cache setup
        
    def map_geolocations(self, input_file: str, output_file: str) -> None:
        # Main processing pipeline
```

Key Features:
- Address normalization
- Geocoding with caching
- Progress tracking
- Statistical reporting

#### StoreDistanceMapper
```python
class StoreDistanceMapper:
    def __init__(self, api_key: str, cache_file: str = 'distance_cache.json'):
        # Initialization with API key and cache setup
        
    def process_orders(self, orders_df: pd.DataFrame, 
                      stores_df: pd.DataFrame, 
                      output_file: str) -> None:
        # Main processing pipeline
```

Key Features:
- Haversine distance calculation
- Driving distance calculation
- Store proximity analysis
- Multi-format reporting

## 3. Implementation Details

### 3.1 API Access Management
```python
def get_secret():
    secret_name = "CommerceReporting/GeoLocation"
    region_name = "eu-west-1"
    # AWS Secrets Manager integration
```

### 3.2 Data Processing Pipeline

#### Phase 1: Geocoding
1. **Address Normalization**
```python
def normalize_text(text: str) -> str:
    # ASCII normalization
    # Special character removal
    # Whitespace standardization
```

2. **Geocoding Process**
```python
def get_lat_long(self, address_components: dict) -> Tuple[Optional[float], Optional[float]]:
    # Address validation
    # Cache checking
    # API call with rate limiting
    # Result caching
```

#### Phase 2: Distance Mapping
1. **Distance Calculation Strategy**
```python
def _find_closest_store(self, order: pd.Series, stores_df: pd.DataFrame) -> Dict:
    # Initial Haversine calculation
    # Optimized store selection
    # Driving distance calculation
```

2. **Range Analysis**
```python
def _find_stores_within_linear(self, order: pd.Series, stores_df: pd.DataFrame) -> List[Dict]:
    # 5km radius check
    # Store filtering
```

### 3.3 Optimization Mechanisms

#### Caching System
1. **Geocoding Cache**
```python
{
    "address_string": [latitude, longitude],
    ...
}
```

2. **Distance Cache**
```python
{
    "origin_destination_key": distance_in_km,
    ...
}
```

#### API Call Reduction
1. **Initial Filtering**
   - Use Haversine distance first
   - Only calculate driving distance for stores within extended linear range
   - Cache all results for reuse

2. **Batch Processing**
   - Progress tracking with tqdm
   - Periodic cache saving
   - Error handling and logging

## 4. Performance KPIs

### 4.1 Processing Metrics
```python
def _log_statistics(self, df: pd.DataFrame, unmapped_df: pd.DataFrame) -> None:
    # Total addresses processed
    # Success rate
    # API call count
    # Cache efficiency
```

### 4.2 Output Format
1. **Coordinates Sheet**
   - Order details
   - Geocoded coordinates
   - Closest store information
   - Distance metrics

2. **5km Stores Linear Sheet**
   - Orders within linear range
   - Store counts
   - Distance measurements

3. **5km Stores Drivable Sheet**
   - Orders within driving range
   - Store counts
   - Actual driving distances

## 5. Error Handling

### 5.1 Logging System
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'geolocation_mapping_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
```

### 5.2 Error Categories
1. **API Errors**
   - Rate limiting
   - Service unavailability
   - Invalid responses

2. **Data Errors**
   - Missing fields
   - Invalid coordinates
   - Format inconsistencies

## 6. Usage Example

```python
def main():
    # Get API key
    api_key = get_secret()
    
    # Phase 1: Geocoding
    mapper = GeoLocationMapper(api_key=api_key)
    mapper.map_geolocations(
        input_file='testing_DE_ORDER.csv',
        output_file='testing_DE_ORDER_mapped.xlsx'
    )
    
    # Phase 2: Distance Mapping
    distance_mapper = StoreDistanceMapper(api_key=api_key)
    distance_mapper.process_orders(
        orders_df=orders_df,
        stores_df=stores_df,
        output_file='F_DE_distance_mapping_results.xlsx'
    )
```

## 7. Maintenance and Support

### 7.1 Cache Management
- Periodic cache cleanup
- Invalid entry removal
- Cache size monitoring

### 7.2 Performance Monitoring
- API call tracking
- Processing time analysis
- Error rate monitoring

### 7.3 Data Quality
- Address validation
- Coordinate verification
- Distance calculation accuracy

## 8. Future Enhancements

### 8.1 Potential Improvements
1. **Processing Optimization**
   - Parallel processing implementation
   - Batch API requests
   - Advanced caching strategies

2. **Feature Additions**
   - Real-time processing capability
   - Additional distance calculation methods
   - Enhanced reporting options

### 8.2 Scalability Considerations
1. **Data Volume Handling**
   - Chunked processing
   - Distributed computation
   - Memory optimization

2. **API Management**
   - Multiple API key support
   - Service provider redundancy
   - Rate limit optimization
